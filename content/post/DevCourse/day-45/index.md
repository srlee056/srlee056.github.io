+++
author = "Seorim"
title =  "Day 45 Airflow(4)"
slug = "day-45"
date = 2023-12-15T12:53:19+09:00

categories = [
    "DevCourse",
]
tags = [
    "TIL", "Airflow", "MySQL", "Redshift" 
]
+++

<style>
g1 { color: #79AC78 }
g2 { color: #B0D9B1 }
g3 { color: #D0E7D2 }
g4 { color: #618264 }
o1 { color: #F9B572 }
w1 { color: #FAF8ED }
</style>

# ğŸ“‹Â ê³µë¶€ ë‚´ìš©

## OLTP í…Œì´ë¸” ë³µì‚¬í•˜ê¸°

-   Production MySQL Tables (OLTP) -> AWS Redshift (OLAP)

<br>

-   MySQL Tables(Source) -> Airflow Server
    1. `COPY Command` : -> S3(Cloud Storage) -> Data Warehouse
    2. `INSERT Command` : -> Data Warehouse

## ì‹¤ìŠµ ê´€ë ¨ ì„¤ì •

### ê¶Œí•œ ì„¤ì •

> ë²„í‚·ê³¼ DB ì„œë²„ë¥¼ ê°€ì§„ ê³„ì •ì—ì„œ ì„¤ì •

**1. Airflow DAGì—ì„œ S3 ì ‘ê·¼ (Write ê¶Œí•œ)**

-   IAM Userë¥¼ ìƒì„±
-   Userì—ê²Œ S3ë²„í‚·ì— ëŒ€í•œ Read/Write ê¶Œí•œì„ ì„¤ì •
-   Userì˜ access key, secret key ì‚¬ìš©

<br>

-   <o1>Custom Policy</o1>

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": ["s3:GetBucketLocation", "s3:ListAllMyBuckets"],
            "Resource": "arn:aws:s3:::*"
        },
        {
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::grepp-data-engineering",
                "arn:aws:s3:::grepp-data-engineering/*"
            ]
        }
    ]
}
```

**2. Redshiftì—ì„œ S3 ì ‘ê·¼ (Read ê¶Œí•œ)**

-   Redshiftì—ì„œ S3 ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” Role ìƒì„± í›„ Redshiftì— ì§€ì •

### Airflow Connection ì„¤ì •

-   S3 : AWS type ì„ íƒ í›„ access key id, secret access key ì…ë ¥ (Extraì— ì§€ì—­ ì…ë ¥)

    ![](image-2.png)

    ![](image-1.png)

-   MySQL : í˜¸ìŠ¤íŠ¸, schema, id/pw, port ì…ë ¥

    ![](image.png)

## DB Tables

### MySQL(OLTP) Table

```sql
CREATE TABLE prod.nps (
    id INT NOT NULL AUTO_INCREMENT primary key,
    created_at timestamp,
    score smallint
);
```

### Redshift(OLAP) Table

-   DAG ì‹¤í–‰ ì´ì „ì— í•´ë‹¹ í…Œì´ë¸”ì„ ë¯¸ë¦¬ ìƒì„±í•´ì•¼ í•¨

```sql
CREATE TABLE {schema}.nps (
    id INT NOT NULL primary key,
    created_at timestamp,
    score smallint
);
```

## Full Refresh

### tasks in DAG

> ì´ë¯¸ ì‘ì„±ë˜ì–´ìˆëŠ” operatorì˜ íŒŒë¼ë¯¸í„°ê°’ë§Œ ì‘ì„±í•˜ì—¬ taskë¥¼ ìƒì„±  
> Incremental Updateë„ ê°™ì€ ë°©ì‹ ì‚¬ìš©

```python
from airflow.providers.amazon.aws.transfers.sql_to_s3 import SqlToS3Operator
from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator
```

<br>

`SqlToS3Operator`

-   MySQL SQL ê²°ê³¼ -> S3
-   ì €ì¥ ìœ„ì¹˜ : s3://s3_bucket/s3_key (s3://grepp-data-engineering/{user_id}-nps)
-   _Task code_

    ```python
    mysql_to_s3_nps = SqlToS3Operator(
        task_id = 'mysql_to_s3_nps',
        query = "SELECT * FROM prod.nps",
        s3_bucket = s3_bucket,
        s3_key = s3_key, #{schema} - {table}
        sql_conn_id = "mysql_conn_id",
        aws_conn_id = "aws_conn_id",
        verify = False,
        replace = True, #íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë®ì–´ì”€
        pd_kwargs={"index": False, "header": False}, #index, headerë¥¼ ì œì™¸í•˜ê³  copy
        dag = dag
        )
    ```

`S3ToRedshiftOperator`

-   S3 -> Redshift í…Œì´ë¸” ( Redshift : {schema}.nps )
-   COPY command is used
-   _Task code_

    ```python
    s3_to_redshift_nps = S3ToRedshiftOperator(
        task_id = 's3_to_redshift_nps',
        s3_bucket = s3_bucket,
        s3_key = s3_key,
        schema = schema,
        table = table,
        copy_options=['csv'],   # file type : csv
        method = 'REPLACE',     # REPLACE : í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë®ì–´ì”€ (full refresh)
        redshift_conn_id = "redshift_dev_db",
        aws_conn_id = "aws_conn_id",
        dag = dag
    )
    ```

## Incremental Update

### MySQL/PostgreSQL Table ì¡°ê±´

-   created(timestamp) : Optional
-   `modified(timestamp)`
-   deleted(boolean) : ë ˆì½”ë“œë¥¼ ì‚­ì œí•˜ì§€ ì•Šê³  `deleted = True`

### êµ¬í˜„ ë°©ì‹

**1. ROW_NUMBERë¡œ ì§ì ‘ êµ¬í˜„**

-   Redshiftì˜ A í…Œì´ë¸”ì„ temp_A í…Œì´ë¸”ë¡œ ë³µì‚¬
-   MySQLì˜ A í…Œì´ë¸” ë ˆì½”ë“œ ì¤‘ `modified == execution_date(ì§€ë‚œ ì¼)`ì¸ ëª¨ë“  ë ˆì½”ë“œë¥¼ temp_Aë¡œ ë³µì‚¬
    -   MySQLì— ë‹¤ìŒ ì¿¼ë¦¬ë¥¼ ë³´ë‚´ê³  ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥. S3ë¡œ ì—…ë¡œë“œí•˜ê³  COPY ìˆ˜í–‰
    ```sql
    SELECT *
        FROM A
        WHERE DATE(modified) = DATE(execution_date)
    ```
-   temp_Aì˜ ë ˆì½”ë“œë“¤ì„ primary keyë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…˜í•œ ë‹¤ìŒ, modified ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ DESC ì •ë ¬
-   ì¼ë ¨ë²ˆí˜¸ê°€ 1ì¸ ê²ƒë“¤ë§Œ ë‹¤ì‹œ Aë¡œ ë³µì‚¬

<br>

**2. UPSERT ì‚¬ìš© (ì‹¤ìŠµì—ì„œ ì‚¬ìš©)**

**`S3ToRedshiftOperator`ë¡œ êµ¬í˜„**

-   query íŒŒë¼ë¯¸í„°
    ```sql
    SELECT *
        FROM A
        WHERE DATE(modified) = DATE(execution_date)
    ```
-   method íŒŒë¼ë¯¸í„° : `â€œUPSERTâ€`
-   upsert_keys íŒŒë¼ë¯¸í„° : Primary key
    -   nps í…Œì´ë¸”ì´ë¼ë©´ â€œidâ€ í•„ë“œë¥¼ ì‚¬ìš©

### tasks in DAG

`SqlToS3Operator`

-   `'{{ execution_date }}'` ë¥¼ ì¿¼ë¦¬ sqlì— ë„£ìŒìœ¼ë¡œì„œ airflowê°€ ë„˜ê²¨ì£¼ëŠ” execution date ê°’ì„ í™œìš©í•  ìˆ˜ ìˆìŒ
-   ì—¬ê¸°ì—ì„œ `modified`ê°€ ì•„ë‹ˆë¼ `created_at`ì¸ ì´ìœ ëŠ” ì• ì´ˆì— í…Œì´ë¸”ì— `modified`ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸

```python
mysql_to_s3_nps = SqlToS3Operator(
    task_id = 'mysql_to_s3_nps',
    query = "SELECT * FROM prod.nps WHERE DATE(created_at) = DATE('{{ execution_date }}')", # query íŒŒë¼ë¯¸í„°ì— ìƒˆë¡œ ìƒì„±ëœ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¤ë„ë¡ sqlì„ ì…ë ¥
    s3_bucket = s3_bucket,
    s3_key = s3_key,
    sql_conn_id = "mysql_conn_id",
    aws_conn_id = "aws_conn_id",
    verify = False,
    replace = True,
    pd_kwargs={"index": False, "header": False},
    dag = dag
)

```

`S3ToRedshiftOperator`

```python
s3_to_redshift_nps = S3ToRedshiftOperator(
    task_id = 's3_to_redshift_nps',
    s3_bucket = s3_bucket,
    s3_key = s3_key,
    schema = schema,
    table = table,
    copy_options=['csv'],
    redshift_conn_id = "redshift_dev_db",
    aws_conn_id = "aws_conn_id",
    method = "UPSERT",     # UPSERT ë°©ì‹ìœ¼ë¡œ í…Œì´ë¸”ì„ ìˆ˜ì •
    upsert_keys = ["id"],  # ì´ í…Œì´ë¸”ì˜ primary keyë¥¼ ê¸°ì¤€ìœ¼ë¡œ upsertë¥¼ ì§„í–‰
    dag = dag
)

```

## ì‹¤ìŠµ ì§„í–‰

-   Full Refresh í…ŒìŠ¤íŠ¸í•´ë³´ê¸°

    ```bash
    airflow dags test MySQL_to_Redshift
    ```

    -   ì‹¤í–‰ë˜ëŠ” sql query
        ```sql
        COPY imsolem1226.nps
        FROM 's3://grepp-data-engineering/imsolem1226-nps'
        credentials
        'aws_access_key_id=xxx;aws_secret_access_key=xxx'
        csv;
        ```
    -   ì‹¤í–‰ ì„±ê³µ í›„ Redshiftì— ì €ì¥ëœ ë°ì´í„° í™•ì¸

    ![](image-3.png)

-   Incremental Update (Upsert) í…ŒìŠ¤íŠ¸í•´ë³´ê¸°
    ```bash
    airflow dags test MySQL_to_Redshift_v2
    ```
    -   ì‹¤í–‰ë˜ëŠ” sql query
        ```sql
        COPY #
        FROM 's3://grepp-data-engineering/imsolem1226-nps'
        credentials
        'aws_access_key_id=xxx;aws_secret_access_key=xxx'
        csv;
        -- ì„ì‹œ í…Œì´ë¸”ê³¼ idê°€ ê°™ì€ ë°ì´í„°ë“¤(ì¤‘ë³µëœ ë°ì´í„°ë“¤)ì„ ëª¨ë‘ ì§€ìš°ê³ , ì„ì‹œ í…Œì´ë¸”ì˜ ë°ì´í„°ë§Œ ë‹¤ì‹œ ì¶”ê°€í•œë‹¤
        DELETE FROM imsolem1226.nps USING #nps WHERE nps.id = #nps.id;
        INSERT INTO imsolem1226.nps SELECT * FROM #nps;
        ```

## Backfill ì‹¤í–‰

-   ë°ì´í„°ë¥¼ ì—¬ëŸ¬ë²ˆ ë‹¤ì‹œ ì½ì–´ì™€ì•¼ í•˜ëŠ” ê²½ìš° í•œë²ˆì— í•˜ë‚˜ì”© vs. í•œë²ˆì— ì—¬ëŸ¬ê°œì”©

1. ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ì´ ë“¤ì–´ê°€ê²Œ ë˜ë©´ ë°ì´í„° ì†ŒìŠ¤ ìª½ì— ë¬¸ì œê°€ ë°œìƒ

-   ë°ì´í„° ì½ê¸°ë¥¼ ì „ë‹´ìœ¼ë¡œ í•˜ëŠ” workerë¥¼ ë§Œë“¤ì–´ ë‘ëŠ” ë°©ì‹ìœ¼ë¡œ ë¶„ì‚°í•˜ì—¬ í•´ê²°
-   ë°ì´í„° ì“°ê¸°ëŠ” main(master?)ì—ì„œ ì§„í–‰

2. ì—¬ëŸ¬ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ê²½ìš°, ë°ì´í„° ë®ì–´ì“°ê¸° ë“± ë¬¸ì œ ë°œìƒ

-   í•˜ë‚˜ì”© ì‹¤í–‰í•˜ëŠ”ê²ƒì´ ì•ˆì „ : `max_active_runs`(DAG parameter)ë¥¼ 1ë¡œ ì„¸íŒ…(?)

### Command Line

```bash
airflow dags backfill dag_id -s 2018-07-01 -e 2018-08-01
```

-   catchUp : Trueë¡œ
-   execution_dateì„ ì‚¬ìš©í•´ì„œ Incremental updateê°€ êµ¬í˜„ë˜ì–´ ìˆìŒ
-   start_dateë¶€í„° ì‹œì‘í•˜ì§€ë§Œ end_dateì€ í¬í•¨í•˜ì§€ ì•ŠìŒ
-   ì‹¤í–‰ìˆœì„œëŠ” ëœë¤ (ë‚ ì§œ, ì‹œê°„ ìˆœ X)
-   ë‚ ì§œìˆœìœ¼ë¡œ í•˜ê³  ì‹¶ë‹¤ë©´ DAG default_argsì˜ depends_on_pastë¥¼ Trueë¡œ ì„¤ì •
    ```python
    default_args = {
        'depends_on_past': True,
        ...
    ```

### How to Make Your DAG Backfill ready

**ëª¨ë“  DAGê°€ backfillì„ í•„ìš”ë¡œ í•˜ì§€ëŠ” ì•ŠìŒ**

-   Full Refreshë¥¼ í•œë‹¤ë©´ backfillì€ ì˜ë¯¸ê°€ ì—†ìŒ
-   Incremental Updateë¥¼ í•´ë„, <g1>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ì¤€ backfill</g1>ì„ í•˜ëŠ” ê²½ìš°ì—ëŠ” execution_dateì„ ì´ìš©í•œ backfillì€ í•„ìš”í•˜ì§€ ì•ŠìŒ (Data Warehouse í…Œì´ë¸”ì— ê¸°ë¡ëœ ì‹œê°„ ê¸°ì¤€)

**backfill êµ¬í˜„ ìƒí™© ë° ìš”ê±´**

-   ë°ì´í„°ê°€ êµ‰ì¥íˆ ì»¤ì§€ë©´ backfill ê¸°ëŠ¥ êµ¬í˜„ì€ í•„ìˆ˜
-   airflowë¥¼ í™œìš©í•˜ëŠ”ê²ƒì´ ë§ì´ ë„ì›€ë¨
-   `ë°ì´í„° ì†ŒìŠ¤ê°€ backfill ë°©ì‹ì„ ì§€ì›`í•˜ëŠ”ê²ƒì´ ì œì¼ ì¤‘ìš”

**backfillì„ ì–´ë–»ê²Œ êµ¬í˜„í•  ê²ƒì¸ê°€?**

-   `execution_date`ì„ ì‚¬ìš©í•´ì„œ ì—…ë°ì´íŠ¸í•  ë°ì´í„° ê²°ì •
-   `catchup` í•„ë“œë¥¼ `True`ë¡œ ì„¤ì •
-   start_date/end_dateì„ backfillí•˜ë ¤ëŠ” ë‚ ì§œë¡œ ì„¤ì •
-   DAGë¥¼ êµ¬í˜„í•  ë•Œ execution_dateì„ ê³ ë ¤í•´ì•¼ í•˜ë©°, `idempotent` í•´ì•¼í•¨

# ğŸ‘€Â CHECK

_<span style = "font-size:15px">(ì–´ë µê±°ë‚˜ ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ê²ƒ ë“± ë‹¤ì‹œ í™•ì¸í•  ê²ƒë“¤)</span>_

# â— ëŠë‚€ ì 
