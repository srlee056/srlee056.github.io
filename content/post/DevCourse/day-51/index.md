+++
author = "Seorim"
title =  "Day 51 Airflow(6)"
slug = "day-51"
date = 2024-01-01T11:44:47+09:00

categories = [
    "DevCourse",
]
tags = [
    "TIL", "Airflow", "dbt", 
]
+++

<style>
g1 { color: #79AC78 }
g2 { color: #B0D9B1 }
g3 { color: #D0E7D2 }
g4 { color: #618264 }
o1 { color: #F9B572 }
w1 { color: #FAF8ED }
</style>

# ğŸ“‹Â ê³µë¶€ ë‚´ìš©

## Airflow Docker í™˜ê²½ì„¤ì •

### docker-compose.yml

- `:-` : if ì²˜ëŸ¼ ì¡°ê±´ë¬¸ ì—­í• 

```yaml
environment:
  AIRFLOW_VAR_DATA_DIR: /opt/airflow/data
  # í™˜ê²½ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´(ì„¸íŒ…ë˜ì§€ ì•Šì•˜ë‹¤ë©´) :- ë’¤ì˜ ì´ë¦„ë“¤ì„ í•´ë‹¹ ë³€ìˆ˜ì— ì¶”ê°€í•˜ë¼ëŠ” ì˜ë¯¸
  _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- yfinance pandas numpy oauth2client gspread}
```

- `DATA_DIR` ë³€ìˆ˜ í™•ì¸í•˜ëŠ” ë°©ë²•

  - Postgres `Named Volume` ì•ˆì— ì •ë³´ê°€ ì €ì¥ë¨
  - Web UIì—ì„œ í™•ì¸ ë¶ˆê°€í•˜ì§€ë§Œ commandë¥¼ í†µí•´ í™•ì¸ ê°€ëŠ¥

  ```bash
  docker exec -it {airflow-scheduler-container-name} airflow variables get DATA_DIR
  ```

  ![](image.png)

- í™˜ê²½ì„¤ì •ê°’(_Variables, Connections_)ì„ ê´€ë¦¬/ë°°í¬í•˜ëŠ” ë°©ë²•

```yaml
x-airflow-common:
  &airflow-common
  ...
  environment:
    &airflow-common-env
    AIRFLOW_VAR_DATA_DIR: /opt/airflow/data
    AIRFLOW_CONN_TEST_ID: test_connection
```

+) í™˜ê²½ ë³€ìˆ˜ê°€ ì•„ë‹Œ, credentials ì „ìš©ìœ¼ë¡œ `Secrets ë°±ì—”ë“œ`ë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•¨ - [ë§í¬](https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/secrets-backend/index.html)

## ELT êµ¬í˜„

- [Build_Summary.py](https://github.com/learndataeng/learn-airflow/blob/main/dags/Build_Summary.py)

```
def get_Redshift_connection():
    hook = PostgresHook(postgres_conn_id = 'redshift_dev_db')
    return hook.get_conn().cursor()

def execSQL(**context):
    schema = context['params']['schema']
    table = context['params']['table']
    select_sql = context['params']['sql']

    ...

    cur = get_Redshift_connection()

    # drop table if exits, before CTAS
    sql = f"""DROP TABLE IF EXISTS {schema}.temp_{table};CREATE TABLE {schema}.temp_{table} AS """
    sql += select_sql
    cur.execute(sql)

    cur.execute(f"""SELECT COUNT(1) FROM {schema}.temp_{table}""")
    count = cur.fetchone()[0]
    if count == 0:
        raise ValueError(f"{schema}.{table} didn't have any record")

    # ì¤‘ë³µ ë ˆì½”ë“œ, Primary key ë“± ì—¬ëŸ¬ê°€ì§€ í…ŒìŠ¤íŠ¸ ì§„í–‰ ê°€ëŠ¥
    # DBTë¥¼ ì‚¬ìš©í•˜ë©´ ì´ëŸ° ì—¬ëŸ¬ê°€ì§€ í…ŒìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì‘ì„±í•  í•„ìš” ì—†ì–´ì§

    ...

# @once : ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ì§€ ì•Šê³  í•œë²ˆë§Œ ì‹¤í–‰ë¨
dag = DAG(
    dag_id = "Build_Summary",
    start_date = datetime(2021,12,10),
    schedule = '@once',
    catchup = False
)

execsql = PythonOperator(
    task_id = 'mau_summary',
    python_callable = execSQL,
    params = {
        'schema' : 'imsolem1226',
        'table': 'mau_summary',
        'sql' : """SELECT
  TO_CHAR(A.ts, 'YYYY-MM') AS month,
  COUNT(DISTINCT B.userid) AS mau
FROM raw_data.session_timestamp A
JOIN raw_data.user_session_channel B ON A.sessionid = B.sessionid
GROUP BY 1
;"""
    },
    dag = dag
)
```

### CTAS SQL query to `config`

- config/mau_summary.py

```python
# Python dictionary í˜•íƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ .py í™•ì¥ìë¥¼ ê°€ì§€ê²Œ ë¨
{
'table': 'mau_summary',
'schema': 'imsolem1226',
'main_sql': """SELECT ...;""",
'input_check': [ ],
'output_check': [ ],
}
```

### ì˜¤í¼ë ˆì´í„° êµ¬í˜„

- dags/plugins/redshift_summary.py - [ë§í¬](https://github.com/learndataeng/learn-airflow/blob/main/dags/plugins/redshift_summary.py#L89)

## ì™¸ë¶€ API ì—°ë™í•˜ì—¬ DAG ì‘ì„±

### êµ¬ê¸€ ì‹œíŠ¸

### Slack

- ì•± ìƒì„± ë° Webhookìœ¼ë¡œ ë©”ì„¸ì§€ ì „ì†¡

```bash
curl -X POST -H 'Content-type: application/json' --data '{"text":"Hello, Slack!"}' https://hooks.slack.com/services/{slack_webhook_url}
```

![](image-1.png)

![](image-2.png)

## Airflow API & ëª¨ë‹ˆí„°ë§

# ğŸ‘€Â CHECK

_<span style = "font-size:15px">(ì–´ë µê±°ë‚˜ ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ê²ƒ ë“± ë‹¤ì‹œ í™•ì¸í•  ê²ƒë“¤)</span>_

# â— ëŠë‚€ ì 
